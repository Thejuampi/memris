# Plan: Run JMH Benchmarks and Update Documentation

## Overview
Run all three JMH benchmarks with default configuration and update performance documentation with new results.

## Files to Modify
- `memris-core/src/jmh/java/io/memris/benchmarks/TestEntityRepositoryBenchmark.java` (lines 34, 37)
- `docs/BENCHMARK_BASELINES.md` (add new results sections)
- `docs/DEVELOPMENT.md` (update benchmark execution commands)
- `README.md` (add benchmark section)

---

## Step 1: Adjust TestEntityRepositoryBenchmark for Capacity Limit

**File:** `memris-core/src/jmh/java/io/memris/benchmarks/TestEntityRepositoryBenchmark.java`

**Changes:**
- Line 34: Change `@Param({ "1000000" })` to `@Param({ "750000" })` for `initialRows`
- Line 37: Change `@Param({ "1000000" })` to `@Param({ "750000" })` for `opsPerInvocation`

**Rationale:** 750,000 rows stays well under 1,048,576 capacity limit with ~286K buffer

---

## Step 2: Run JMH Benchmarks

**Command (from repository root):**
```bash
mvn -pl memris-core pw.krejci:jmh-maven-plugin:0.2.2:benchmark
```

**Configuration (from pom.xml:133-136):**
- Warmup iterations: 2
- Measurement iterations: 3
- Forks: 1

**Expected benchmarks to run:**
1. `MemrisBenchmarks` - Core single-threaded (count, insert, lookup, scan)
2. `ConcurrentReadWriteBenchmark` - Multi-threaded (various thread configs)
3. `TestEntityRepositoryBenchmark` - Mixed workload (750K rows)

**Capture output:**
```bash
mvn -pl memris-core pw.krejci:jmh-maven-plugin:0.2.2:benchmark > benchmark-results-20260201.txt 2>&1
```

---

## Step 3: Analyze Benchmark Results

**Key metrics to extract from output:**

**MemrisBenchmarks:**
- `insert_100k_rows` - Average time (ms/op) and throughput
- `lookup_by_id` - Average time (ms/op) and throughput
- `scan_all_rows` - Average time (ms/op) and scan rate
- `count_rows` - Average time (ms/op)

**ConcurrentReadWriteBenchmark:**
- `write1_read1.writer_1thread` and `reader_1thread` throughput
- `write2_read2.writer_2threads` and `reader_2threads` throughput
- `write4_read4.writer_4threads` and `reader_4threads` throughput
- `write8_read8.writer_8threads` and `reader_8threads` throughput
- `write4_read8.writer_4threads_read8` and `reader_8threads_write4` throughput
- `write8_read4.writer_8threads_read4` and `reader_4threads_write8` throughput

**TestEntityRepositoryBenchmark:**
- `mixedSaveUpdateDelete` - Total throughput (ops/sec)

**Validation:**
- Ensure no capacity errors
- Verify throughput values are reasonable (no negatives or extreme outliers)
- Check that all three benchmarks completed successfully

---

## Step 4: Update BENCHMARK_BASELINES.md

**File:** `docs/BENCHMARK_BASELINES.md`

**Updates:**

1. **Update header date (line 3):**
   - Change to actual run date: `**Generated:** 2026-02-01`

2. **Update test data note (line 6):**
   - Change to: `**Test Data:** 100,000 pre-populated rows (except TestEntityRepositoryBenchmark uses 750,000 rows)`

3. **Add TestEntityRepositoryBenchmark Results section after existing detailed results:**

```markdown
---

## TestEntityRepositoryBenchmark Results

**What it measures:** Mixed workload (40% save, 40% update, 20% delete) with 750,000 pre-populated rows

```
Benchmark: io.memris.benchmarks.TestEntityRepositoryBenchmark.mixedSaveUpdateDelete
Mode: Throughput
Iterations: 3
Warmup: 2 iterations
Fork: 1

Result:
  [ACTUAL_VALUE] ± [ACTUAL_ERROR] ops/sec [Average]
  (min, avg, max) = ([MIN], [AVG], [MAX])
  stdev = [STDDEV]
  CI (99.9%): [[CI_LOWER], [CI_UPPER]]
```

**Analysis:**
- **Throughput:** [ACTUAL_VALUE] ops/sec
- **Row capacity:** 750,000 (within 1,048,576 limit)
- **Workload distribution:** 40% save, 40% update, 20% delete
```

4. **Add ConcurrentReadWriteBenchmark Results section after TestEntityRepositoryBenchmark:**

```markdown
---

## ConcurrentReadWriteBenchmark Results

**What it measures:** Multi-threaded concurrent read/write with 100,000 pre-populated rows

**Writer workload:** 50% insert, 30% update, 20% delete
**Reader workload:** 60% lookup by ID, 30% count, 10% scan all

### Thread Configuration Results

| Configuration | Write Throughput | Read Throughput | Total Throughput | Notes |
|---------------|------------------|-----------------|------------------|-------|
| 1 Writer + 1 Reader | [VALUE] ± [ERROR] | [VALUE] ± [ERROR] | [TOTAL] | Balanced workload |
| 2 Writers + 2 Readers | [VALUE] ± [ERROR] | [VALUE] ± [ERROR] | [TOTAL] | |
| 4 Writers + 4 Readers | [VALUE] ± [ERROR] | [VALUE] ± [ERROR] | [TOTAL] | May show write contention |
| 8 Writers + 8 Readers | [VALUE] ± [ERROR] | [VALUE] ± [ERROR] | [TOTAL] | Expected write starvation |
| 4 Writers + 8 Readers | [VALUE] ± [ERROR] | [VALUE] ± [ERROR] | [TOTAL] | Read-heavy |
| 8 Writers + 4 Readers | [VALUE] ± [ERROR] | [VALUE] ± [ERROR] | [TOTAL] | Write-heavy |

**Analysis:**
- Read operations scale well with thread count
- Write operations may show degradation at higher thread counts due to seqlock contention
- Known limitation: Seqlock-based concurrency can cause writer starvation in write-heavy workloads
```

5. **Update Summary Table (around line 12):**
   - Add row for TestEntityRepositoryBenchmark
   - Add row for ConcurrentReadWriteBenchmark (representative config)

```markdown
| mixed_workload | thrpt | [VALUE] | ± [ERROR] | ops/sec | 750K rows, 40% save/40% update/20% delete |
| concurrent_rw_1w1r | thrpt | [TOTAL] | ± [ERROR] | ops/sec | 100K rows, 1 writer + 1 reader |
```

6. **Update "Running the Benchmarks" section (lines 126-134):**
```bash
# Run all benchmarks with default configuration
cd memris-core
mvn pw.krejci:jmh-maven-plugin:0.2.2:benchmark

# Run specific benchmark class
mvn pw.krejci:jmh-maven-plugin:0.2.2:benchmark -Djmh.includeClass=io.memris.benchmarks.MemrisBenchmarks
```

---

## Step 5: Update DEVELOPMENT.md

**File:** `docs/DEVELOPMENT.md`

**Updates:**

1. **Update lines 299-303 (benchmark execution commands):**
```bash
# Run all benchmarks with default configuration
cd memris-core
mvn pw.krejci:jmh-maven-plugin:0.2.2:benchmark

# Run specific benchmark
mvn pw.krejci:jmh-maven-plugin:0.2.2:benchmark -Djmh.includeClass=io.memris.benchmarks.MemrisBenchmarks
```

2. **Update lines 321-324 (concurrent benchmark command):**
```bash
# Run concurrent benchmark
cd memris-core
mvn pw.krejci:jmh-maven-plugin:0.2.2:benchmark -Djmh.includeClass=io.memris.benchmarks.ConcurrentReadWriteBenchmark
```

3. **Add note after line 324:**
```bash
# Note: For detailed benchmark results and analysis, see docs/BENCHMARK_BASELINES.md
```

4. **Keep existing baseline data tables** (lines 293-317) as they provide historical context

---

## Step 6: Update README.md

**File:** `README.md`

**Add new section after line 341 (after "Building" section, before "Maven Dependency"):**

```markdown
## Benchmarks

JMH microbenchmarks measure Memris performance across various workloads.

**Run all benchmarks:**
```bash
mvn -pl memris-core pw.krejci:jmh-maven-plugin:0.2.2:benchmark
```

**Run specific benchmark:**
```bash
# Core single-threaded benchmarks
mvn -pl memris-core pw.krejci:jmh-maven-plugin:0.2.2:benchmark -Djmh.includeClass=io.memris.benchmarks.MemrisBenchmarks

# Concurrent read/write benchmarks
mvn -pl memris-core pw.krejci:jmh-maven-plugin:0.2.2:benchmark -Djmh.includeClass=io.memris.benchmarks.ConcurrentReadWriteBenchmark

# Mixed workload benchmark
mvn -pl memris-core pw.krejci:jmh-maven-plugin:0.2.2:benchmark -Djmh.includeClass=io.memris.benchmarks.TestEntityRepositoryBenchmark
```

**Benchmark Suites:**

| Benchmark | Description | Configuration |
|-----------|-------------|---------------|
| `MemrisBenchmarks` | Single-threaded core operations (count, insert, lookup, scan) | 100K rows |
| `ConcurrentReadWriteBenchmark` | Multi-threaded concurrent read/write patterns | 100K initial rows |
| `TestEntityRepositoryBenchmark` | Mixed workload (40% save, 40% update, 20% delete) | 750K rows |

**Detailed Results:** See [BENCHMARK_BASELINES.md](docs/BENCHMARK_BASELINES.md) for performance baselines, detailed analysis, and comparison with typical RDBMS and in-memory databases.
```

---

## Step 7: Handle Potential Issues

**If benchmarks fail with capacity error:**
1. Reduce TestEntityRepositoryBenchmark rows further (try 500,000)
2. Verify error message in output
3. Document the failure in BENCHMARK_BASELINES.md

**If benchmarks hang or timeout:**
1. Increase JVM heap: `mvn -pl memris-core pw.krejci:jmh-maven-plugin:0.2.2:benchmark -Djmh.jvmArgs="-Xmx4g"`
2. Reduce measurement time in pom.xml (lines 133-136)

**If write operations show 0 throughput (expected):**
1. This is a known limitation documented in CONCURRENCY.md
2. Document in results with explanation of seqlock contention
3. No action needed - this is expected behavior

---

## Verification

After completing all changes:

1. **Verify benchmarks ran successfully:**
   - Check for `BUILD SUCCESS` in output
   - Confirm all three benchmark classes appear in output
   - Verify no capacity errors

2. **Verify documentation updates:**
   - `BENCHMARK_BASELINES.md` contains new results sections
   - `DEVELOPMENT.md` has updated JMH commands
   - `README.md` has new benchmark section

3. **Validate results:**
   - Insert throughput > 100K ops/sec
   - Lookup throughput > 400K ops/sec
   - No negative values or extreme outliers
   - Concurrent read scaling is visible

4. **Test documentation links:**
   - README.md link to BENCHMARK_BASELINES.md works
   - DEVELOPMENT.md references are consistent

---

## Execution Order

1. Edit `TestEntityRepositoryBenchmark.java` (reduce rows to 750K)
2. Run JMH benchmarks and capture output
3. Update `docs/BENCHMARK_BASELINES.md` with all new results
4. Update `docs/DEVELOPMENT.md` with JMH commands
5. Update `README.md` with benchmark section
6. Verify all documentation is consistent and links work
